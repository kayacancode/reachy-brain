OPENAI_API_KEY=
MODEL_NAME="gpt-realtime"

# Local vision model (only used with --local-vision CLI flag)
# By default, vision is handled by gpt-realtime when the camera tool is used
LOCAL_VISION_MODEL=HuggingFaceTB/SmolVLM2-2.2B-Instruct

# Cache for local VLM (only used with --local-vision CLI flag)
HF_HOME=./cache

# Hugging Face token for accessing datasets/models
HF_TOKEN=

# To select a specific profile with custom instructions and tools, to be placed in profiles/<myprofile>/__init__.py
REACHY_MINI_CUSTOM_PROFILE="example"

# ============================================
# Clawdbot Mode (use with --clawdbot flag)
# ============================================

# Clawdbot / OpenClaw endpoint
CLAWDBOT_ENDPOINT="http://localhost:18789/v1/chat/completions"
CLAWDBOT_TOKEN=
CLAWDBOT_MODEL="claude-sonnet-4-20250514"

# ElevenLabs TTS (required for --clawdbot)
ELEVENLABS_API_KEY=
ELEVENLABS_VOICE_ID="21m00Tcm4TlvDq8ikWAM"

# Honcho memory (optional, but recommended for --clawdbot)
HONCHO_API_KEY=
HONCHO_WORKSPACE_ID="reachy-mini"
